{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreea003/beta-version/blob/main/Alzheimer_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "n-i4n93k5tsV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40ac3c2f-3ad0-4a10-d806-a5767c88334c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 'ADNI.zip' -d '/content/Data/'\n",
        "# !unzip '/content/drive/MyDrive/generated_augmented.zip' -d '/content/Data/'"
      ],
      "metadata": {
        "id": "o9Nds9AW7N4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a14b34f4-2d20-440f-ec18-040b02b63041",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ADNI.zip\n",
            "   creating: /content/Data/ADNI/AD/\n",
            "  inflating: /content/Data/ADNI/AD/1.png  \n",
            "  inflating: /content/Data/ADNI/AD/10.png  \n",
            "  inflating: /content/Data/ADNI/AD/11.png  \n",
            "  inflating: /content/Data/ADNI/AD/12.png  \n",
            "  inflating: /content/Data/ADNI/AD/13.png  \n",
            "  inflating: /content/Data/ADNI/AD/14.png  \n",
            "  inflating: /content/Data/ADNI/AD/15.png  \n",
            "  inflating: /content/Data/ADNI/AD/16.png  \n",
            "  inflating: /content/Data/ADNI/AD/17.png  \n",
            "  inflating: /content/Data/ADNI/AD/18.png  \n",
            "  inflating: /content/Data/ADNI/AD/19.png  \n",
            "  inflating: /content/Data/ADNI/AD/2.png  \n",
            "  inflating: /content/Data/ADNI/AD/20.png  \n",
            "  inflating: /content/Data/ADNI/AD/21.png  \n",
            "  inflating: /content/Data/ADNI/AD/22.png  \n",
            "  inflating: /content/Data/ADNI/AD/23.png  \n",
            "  inflating: /content/Data/ADNI/AD/24.png  \n",
            "  inflating: /content/Data/ADNI/AD/25.png  \n",
            "  inflating: /content/Data/ADNI/AD/26.png  \n",
            "  inflating: /content/Data/ADNI/AD/27.png  \n",
            "  inflating: /content/Data/ADNI/AD/28.png  \n",
            "  inflating: /content/Data/ADNI/AD/29.png  \n",
            "  inflating: /content/Data/ADNI/AD/3.png  \n",
            "  inflating: /content/Data/ADNI/AD/30.png  \n",
            "  inflating: /content/Data/ADNI/AD/4.png  \n",
            "  inflating: /content/Data/ADNI/AD/5.png  \n",
            "  inflating: /content/Data/ADNI/AD/6.png  \n",
            "  inflating: /content/Data/ADNI/AD/7.png  \n",
            "  inflating: /content/Data/ADNI/AD/8.png  \n",
            "  inflating: /content/Data/ADNI/AD/9.png  \n",
            "   creating: /content/Data/ADNI/HC/\n",
            "  inflating: /content/Data/ADNI/HC/1.png  \n",
            "  inflating: /content/Data/ADNI/HC/10.png  \n",
            "  inflating: /content/Data/ADNI/HC/2.png  \n",
            "  inflating: /content/Data/ADNI/HC/3.png  \n",
            "  inflating: /content/Data/ADNI/HC/4.png  \n",
            "  inflating: /content/Data/ADNI/HC/5.png  \n",
            "  inflating: /content/Data/ADNI/HC/6.png  \n",
            "  inflating: /content/Data/ADNI/HC/7.png  \n",
            "  inflating: /content/Data/ADNI/HC/8.png  \n",
            "  inflating: /content/Data/ADNI/HC/9.png  \n",
            "  inflating: /content/Data/ADNI/HC/hemlo (1).png  \n",
            "  inflating: /content/Data/ADNI/HC/hemlo (10).png  \n",
            "  inflating: /content/Data/ADNI/HC/hemlo (2).png  \n",
            "  inflating: /content/Data/ADNI/HC/hemlo (3).png  \n",
            "  inflating: /content/Data/ADNI/HC/hemlo (4).png  \n",
            "  inflating: /content/Data/ADNI/HC/hemlo (5).png  \n",
            "  inflating: /content/Data/ADNI/HC/hemlo (6).png  \n",
            "  inflating: /content/Data/ADNI/HC/hemlo (7).png  \n",
            "  inflating: /content/Data/ADNI/HC/hemlo (8).png  \n",
            "  inflating: /content/Data/ADNI/HC/hemlo (9).png  \n",
            "  inflating: /content/Data/ADNI/HC/rename (1).png  \n",
            "  inflating: /content/Data/ADNI/HC/rename (10).png  \n",
            "  inflating: /content/Data/ADNI/HC/rename (2).png  \n",
            "  inflating: /content/Data/ADNI/HC/rename (3).png  \n",
            "  inflating: /content/Data/ADNI/HC/rename (4).png  \n",
            "  inflating: /content/Data/ADNI/HC/rename (5).png  \n",
            "  inflating: /content/Data/ADNI/HC/rename (6).png  \n",
            "  inflating: /content/Data/ADNI/HC/rename (7).png  \n",
            "  inflating: /content/Data/ADNI/HC/rename (8).png  \n",
            "  inflating: /content/Data/ADNI/HC/rename (9).png  \n",
            "   creating: /content/Data/ADNI/MCI/\n",
            "  inflating: /content/Data/ADNI/MCI/1.png  \n",
            "  inflating: /content/Data/ADNI/MCI/10.png  \n",
            "  inflating: /content/Data/ADNI/MCI/2.png  \n",
            "  inflating: /content/Data/ADNI/MCI/3.png  \n",
            "  inflating: /content/Data/ADNI/MCI/4.png  \n",
            "  inflating: /content/Data/ADNI/MCI/5.png  \n",
            "  inflating: /content/Data/ADNI/MCI/6.png  \n",
            "  inflating: /content/Data/ADNI/MCI/7.png  \n",
            "  inflating: /content/Data/ADNI/MCI/8.png  \n",
            "  inflating: /content/Data/ADNI/MCI/9.png  \n",
            "  inflating: /content/Data/ADNI/MCI/anuvab (1).png  \n",
            "  inflating: /content/Data/ADNI/MCI/anuvab (10).png  \n",
            "  inflating: /content/Data/ADNI/MCI/anuvab (2).png  \n",
            "  inflating: /content/Data/ADNI/MCI/anuvab (3).png  \n",
            "  inflating: /content/Data/ADNI/MCI/anuvab (4).png  \n",
            "  inflating: /content/Data/ADNI/MCI/anuvab (5).png  \n",
            "  inflating: /content/Data/ADNI/MCI/anuvab (6).png  \n",
            "  inflating: /content/Data/ADNI/MCI/anuvab (7).png  \n",
            "  inflating: /content/Data/ADNI/MCI/anuvab (8).png  \n",
            "  inflating: /content/Data/ADNI/MCI/anuvab (9).png  \n",
            "  inflating: /content/Data/ADNI/MCI/sad (1).png  \n",
            "  inflating: /content/Data/ADNI/MCI/sad (10).png  \n",
            "  inflating: /content/Data/ADNI/MCI/sad (2).png  \n",
            "  inflating: /content/Data/ADNI/MCI/sad (3).png  \n",
            "  inflating: /content/Data/ADNI/MCI/sad (4).png  \n",
            "  inflating: /content/Data/ADNI/MCI/sad (5).png  \n",
            "  inflating: /content/Data/ADNI/MCI/sad (6).png  \n",
            "  inflating: /content/Data/ADNI/MCI/sad (7).png  \n",
            "  inflating: /content/Data/ADNI/MCI/sad (8).png  \n",
            "  inflating: /content/Data/ADNI/MCI/sad (9).png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir '/content/Data/checkpoint'"
      ],
      "metadata": {
        "id": "rff3gJNSU1IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import tqdm\n",
        "from itertools import repeat\n",
        "\n",
        "PATH = '/content/Data/'\n",
        "ORIG_PATH = '/content/Data/ADNI/'\n",
        "# GEN_PATH = '/content/Data/generated_augmented/'"
      ],
      "metadata": {
        "id": "1wLH4HluOvK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the target image size (e.g., 224x224 for ViT-B/32)\n",
        "target_image_size = (224, 224)\n",
        "\n",
        "# Initialize empty lists for data and labels\n",
        "y = []\n",
        "z = []\n",
        "\n",
        "# Load images and labels from subdirectories within ORIG_PATH\n",
        "for subdirectory in ['AD', 'HC', 'MCI']:  # List of subdirectory names\n",
        "    subdirectory_path = ORIG_PATH + subdirectory + '/'\n",
        "    image_files = glob.glob(subdirectory_path + '*')\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"No images found in {subdirectory_path}\")\n",
        "        continue\n",
        "\n",
        "    for img in tqdm.tqdm(image_files, desc=f'Loading {subdirectory}'):\n",
        "        img_data = cv2.imread(img)\n",
        "        if img_data is not None:\n",
        "            # Resize the image to the target size\n",
        "            img_data = cv2.resize(img_data, target_image_size)\n",
        "            z.append(img_data)\n",
        "            y.append(subdirectory)  # Use the subdirectory name as the label\n",
        "\n",
        "# Split data into training and testing sets (e.g., 80% training, 20% testing)\n",
        "x_train, x_test, y_train, y_test = train_test_split(z, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print shapes of the training and testing sets\n",
        "print(\"Train\", np.array(x_train).shape)\n",
        "print(\"Train\", np.array(y_train).shape)\n",
        "np.save(PATH + 'x_train.npy', np.array(x_train))\n",
        "np.save(PATH + 'y_train.npy', np.array(y_train))\n",
        "\n",
        "print(\"Test\", np.array(x_test).shape)\n",
        "print(\"Test\", np.array(y_test).shape)\n",
        "np.save(PATH + 'x_test.npy', np.array(x_test))\n",
        "np.save(PATH + 'y_test.npy', np.array(y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYZyQkBjQ_OD",
        "outputId": "673f44a2-fad3-4c37-b2f1-64f5ac50440d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading AD: 100%|██████████| 30/30 [00:00<00:00, 225.17it/s]\n",
            "Loading HC: 100%|██████████| 30/30 [00:00<00:00, 407.15it/s]\n",
            "Loading MCI: 100%|██████████| 30/30 [00:00<00:00, 458.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (72, 224, 224, 3)\n",
            "Train (72,)\n",
            "Test (18, 224, 224, 3)\n",
            "Test (18,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREPROCESSING**"
      ],
      "metadata": {
        "id": "iqQphww48V3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VIT TRAINING**"
      ],
      "metadata": {
        "id": "7XdYgXTG-VK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "oMiyY5MABLO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04706dd0-09d4-4e8d-d71b-56e8dfec9714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Uh3pkN4yaNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VIT TRAINED ON ORIGINAL DATASET**"
      ],
      "metadata": {
        "id": "VouBa8ew-oA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Load an example image\n",
        "image = Image.open(\"/content/Data/ADNI/AD/5.png\")  # Replace \"example.jpg\" with your image file path\n",
        "\n",
        "# Get image dimensions\n",
        "width, height = image.size\n",
        "channels = 3  # Assuming it's an RGB image\n",
        "\n",
        "# Set the input shape\n",
        "input_shape = (height, width, channels)\n",
        "print(\"Input shape:\", input_shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsiK3ZTuHAUJ",
        "outputId": "d1aec14c-ae69-4c46-f286-05ad1150d125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (256, 170, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "num_classes = 3\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "x_train,y_train = np.load(PATH+'x_train.npy'), np.load(PATH+'y_train.npy')\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "print(x_train.shape,y_train.shape)\n",
        "\n",
        "x_test,y_test = np.load(PATH+'x_test.npy'), np.load(PATH+'y_test.npy')\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "y_test = label_encoder.fit_transform(y_test)\n",
        "print(x_test.shape,y_test.shape)\n",
        "\n",
        "X = np.concatenate((x_train,x_test),axis=0)\n",
        "y = np.concatenate((y_train,y_test),axis=0)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.1)\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 72  # We'll resize input images to this size\n",
        "patch_size = 6   # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n",
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n",
        "\n",
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = PATH+\"/checkpoint/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    earlyStoppingCallback = keras.callbacks.EarlyStopping(patience=120, monitor='val_accuracy', restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback, earlyStoppingCallback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "vit_classifier = create_vit_classifier()\n",
        "history = run_experiment(vit_classifier)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "D16bhVi11lPZ",
        "outputId": "675b3335-5d4c-426b-cbf0-bd1513149835"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow_addons'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-19d42986fb7e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_addons'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "history_dict = history.history\n",
        "json.dump(history_dict, open('/content/Data/checkpoint/history_generated', 'w'))"
      ],
      "metadata": {
        "id": "v4kNaYufQkWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, RocCurveDisplay, auc\n",
        "import seaborn as sns\n",
        "y_predict = vit_classifier.predict(x_test)\n",
        "y_pred = np.argmax(np.round(y_predict), axis=1)\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred))\n",
        "print('\\nClassification Report:\\n', classification_report(y_test, y_pred))\n",
        "confusion_matrix_graph = confusion_matrix(y_test, y_pred)\n",
        "print(confusion_matrix_graph)"
      ],
      "metadata": {
        "id": "G598pb1Z_Uyu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "outputId": "a20e6d5d-5127-4c67-b4f6-a35b6f5f0e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         3\n",
            "           1       1.00      0.75      0.86         4\n",
            "           2       0.67      1.00      0.80         2\n",
            "\n",
            "    accuracy                           0.89         9\n",
            "   macro avg       0.89      0.92      0.89         9\n",
            "weighted avg       0.93      0.89      0.89         9\n",
            "\n",
            "[[3 0 0]\n",
            " [0 3 1]\n",
            " [0 0 2]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAGiCAYAAAB6c8WBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdfUlEQVR4nO3df5CVZdk48OtgsmABxRC7/DKZoQRTwBBlcRR8QxlylG0mY5y+LRraNC2ORj+3t7dVm+9sZuaPgUTr1c36kqYFFlmGGPAS6xgIM2ITZZqUsatULrIDC3HO94/m3dyHXdhDB8/R+/Nxnj/2Oc9z3zfOzpxrr+u6nydXKBQKAQAka0C5FwAAlJdgAAASJxgAgMQJBgAgcYIBAEicYAAAEicYAIDECQYAIHGCAQBInGAAABInGACACnHnnXfG5MmTY+jQoTF06NCora2Nn/3sZ0e858EHH4yJEyfGoEGD4owzzohHHnmk6HkFAwBQIcaOHRtf/epXY8uWLbF58+b4j//4j5g/f34888wzvV6/adOmuPzyy2PRokWxdevWqKuri7q6uti+fXtR8+a8qAgAKtfw4cPj5ptvjkWLFh322YIFC6KzszNWr17dfW7GjBkxderUWL58eb/nkBkAgOOoq6sr9uzZ0+Po6uo66n2HDh2K+++/Pzo7O6O2trbXa1pbW2POnDk9zs2dOzdaW1uLWuNbirr6ODq4+7lyL4EKMnj0eeVeAlDB/nHgxeM6fim/k5qX3hc33HBDj3NNTU1x/fXX93r9008/HbW1tbF///5429veFitXrozTTjut12vb2tqiurq6x7nq6upoa2srao0VEwwAQMXIHyrZUI2NjbFkyZIe56qqqvq8/tRTT41t27ZFR0dHPPTQQ7Fw4cJYv359nwFBKQgGAOA4qqqqOuKXf9bAgQNjwoQJERExbdq0+PWvfx2333573HXXXYddW1NTE+3t7T3Otbe3R01NTVFr1DMAAFmFfOmOf1M+n++zx6C2tjbWrl3b49yaNWv67DHoi8wAAGTl//0v8WPR2NgY8+bNi5NPPjleffXVWLFiRaxbty4effTRiIior6+PMWPGRHNzc0REXHvttTFr1qy45ZZb4uKLL477778/Nm/eHHfffXdR8woGACCjUIK/6I/FSy+9FPX19bFr164YNmxYTJ48OR599NG48MILIyJi586dMWDAv5L6M2fOjBUrVsSXvvSl+OIXvxjvfve7Y9WqVXH66acXNW/FPGfAbgJey24C4EiO926CA3/p/SE/x2Lg6PeWbKzjRWYAALLKVCYoF8EAAGSVqUxQLnYTAEDiZAYAIKuEDx16IxAMAECWMgEAkBKZAQDIspsAANJWrocOlYsyAQAkTmYAALKUCQAgcYmVCQQDAJCV2HMG9AwAQOJkBgAgS5kAABKXWAOhMgEAJE5mAACylAkAIHHKBABASmQGACCjUEjrOQOCAQDISqxnQJkAABInMwAAWYk1EAoGACArsTKBYAAAsryoCABIicwAAGQpEwBA4hJrIFQmAIDEyQwAQJYyAQAkTpkAAEiJzAAAZCWWGRAMAEBGam8tVCYAgMTJDABAljIBACTO1kIASFximQE9AwCQOJkBAMhSJgCAxCkTAAApkRkAgCxlAgBInDIBAJASmQEAyEosMyAYAICsxHoGlAkAIHEyAwCQpUwAAIlLrEwgGACArMQyA3oGAKBCNDc3x/Tp02PIkCExcuTIqKurix07dhzxnpaWlsjlcj2OQYMGFTVv0ZmB3bt3xz333BOtra3R1tYWERE1NTUxc+bMuOKKK+Kd73xnsUMCQGUpU5lg/fr10dDQENOnT49//OMf8cUvfjEuuuii+M1vfhNvfetb+7xv6NChPYKGXC5X1LxFBQO//vWvY+7cuXHSSSfFnDlz4j3veU9ERLS3t8cdd9wRX/3qV+PRRx+Ns84664jjdHV1RVdXV49zA7q6oqqqqqjFA8BxUcIyQW/feVVVVb1+5/385z/v8XNLS0uMHDkytmzZEueff36fc+RyuaipqTnmNRZVJrjmmmvisssuiz/96U/R0tISN910U9x0003R0tISO3fujA996ENxzTXXHHWc5ubmGDZsWI/jptuXH/M/AgAqVW/fec3Nzf26t6OjIyIihg8ffsTr9u7dG+9617ti3LhxMX/+/HjmmWeKWmOuUCgU+nvx4MGDY+vWrTFx4sReP//tb38bZ555Zuzbt++I4/SaGXj1RZkBug0efV65lwBUsH8cePG4jr/vBzeWbKwB8z/f78zAa+Xz+bj00kvjlVdeiY0bN/Z5XWtra/z+97+PyZMnR0dHR3z961+PDRs2xDPPPBNjx47t1xqLKhPU1NTEk08+2Wcw8OSTT0Z1dfVRx+ntf8LBA7uLWQoAHD/9/zv5qPrzxd+bhoaG2L59+xEDgYiI2traqK2t7f555syZMWnSpLjrrrviK1/5Sr/mKioY+MxnPhMf//jHY8uWLfH+97+/+4u/vb091q5dG9/61rfi61//ejFDAgAZixcvjtWrV8eGDRv6/df9/zrxxBPjzDPPjGeffbbf9xQVDDQ0NMSIESPi1ltvjW9+85tx6NChiIg44YQTYtq0adHS0hIf/vCHi1o0AFScMj1noFAoxDXXXBMrV66MdevWxfjx44se49ChQ/H000/HBz7wgX7fU/TWwgULFsSCBQvi4MGDsXv3P1P7I0aMiBNPPLHYoQCgMpUpGGhoaIgVK1bEww8/HEOGDOnewj9s2LAYPHhwRETU19fHmDFjupsQb7zxxpgxY0ZMmDAhXnnllbj55pvjhRdeiKuuuqrf8x7zEwhPPPHEGDVq1LHeDgBk3HnnnRERMXv27B7n77333rjiiisiImLnzp0xYMC/NgP+/e9/j6uvvjra2triHe94R0ybNi02bdoUp512Wr/nLWo3wfF0cPdz5V4CFcRuAuBIjvtugu/9Z8nGGvx//m/JxjpevJsAALISezeBYAAAsiojaf668aIiAEiczAAAZCkTAEDiEgsGlAkAIHEyAwCQVUgrMyAYAICMQt5uAgAgITIDAJCVWAOhYAAAshLrGVAmAIDEyQwAQFZiDYSCAQDI0jMAAIlLLBjQMwAAiZMZAICsxF5hLBgAgCxlAgAgJTIDAJBlayEAJM4TCAGAlMgMAECWMgEApK1gNwEAkBKZAQDIUiYAgMQltptAMAAAWYllBvQMAEDiZAYAICux3QSCAQDIUiYAAFIiMwAAWXYTAEDilAkAgJTIDABARmrvJhAMAECWMgEAkBKZAQDISiwzIBgAgCxbCwEgcYllBvQMAEDiZAYAIKOQWGZAMAAAWYkFA8oEAJA4mQEAyPIEQgBInDIBAJASmQEAyEosMyAYAICMQiGtYECZAAAqRHNzc0yfPj2GDBkSI0eOjLq6utixY8dR73vwwQdj4sSJMWjQoDjjjDPikUceKWpewQAAZOULpTuKsH79+mhoaIgnnngi1qxZEwcPHoyLLrooOjs7+7xn06ZNcfnll8eiRYti69atUVdXF3V1dbF9+/Z+z5srVEgu5ODu58q9BCrI4NHnlXsJQAX7x4EXj+v4exZdWLKxhv73mmO+9+WXX46RI0fG+vXr4/zzz+/1mgULFkRnZ2esXr26+9yMGTNi6tSpsXz58n7NIzMAABmFfKFkR1dXV+zZs6fH0dXV1a91dHR0RETE8OHD+7ymtbU15syZ0+Pc3Llzo7W1td//3oppIPSXIK+17y//U+4lUEG+Nu2/yr0EOGbNzc1xww039DjX1NQU119//RHvy+fzcd1118W5554bp59+ep/XtbW1RXV1dY9z1dXV0dbW1u81VkwwAAAVo4RbCxsbG2PJkiU9zlVVVR31voaGhti+fXts3LixZGvpi2AAALJK+DTiqqqqfn35v9bixYtj9erVsWHDhhg7duwRr62pqYn29vYe59rb26Ompqbf8+kZAIAKUSgUYvHixbFy5cp4/PHHY/z48Ue9p7a2NtauXdvj3Jo1a6K2trbf88oMAEBGoUxPIGxoaIgVK1bEww8/HEOGDOmu+w8bNiwGDx4cERH19fUxZsyYaG5ujoiIa6+9NmbNmhW33HJLXHzxxXH//ffH5s2b4+677+73vDIDAJBVpucM3HnnndHR0RGzZ8+OUaNGdR8PPPBA9zU7d+6MXbt2df88c+bMWLFiRdx9990xZcqUeOihh2LVqlVHbDrMkhkAgArRn0f/rFu37rBzl112WVx22WXHPK9gAACySthA+EYgGACAjHL1DJSLngEASJzMAABkKRMAQNpSKxMIBgAgK7HMgJ4BAEiczAAAZBQSywwIBgAgK7FgQJkAABInMwAAGcoEAJC6xIIBZQIASJzMAABkKBMAQOIEAwCQuNSCAT0DAJA4mQEAyCrkyr2C15VgAAAylAkAgKTIDABARiGvTAAASVMmAACSIjMAABkFuwkAIG3KBABAUmQGACDDbgIASFyhUO4VvL4EAwCQkVpmQM8AACROZgAAMlLLDAgGACAjtZ4BZQIASJzMAABkKBMAQOJSexyxMgEAJE5mAAAyUns3gWAAADLyygQAQEpkBgAgI7UGQsEAAGTYWggAifMEQgAgKTIDAJChTAAAibO1EABIiswAAGTYWggAibObAABIiswAAGSk1kAoGACAjNR6BpQJAKBCbNiwIS655JIYPXp05HK5WLVq1RGvX7duXeRyucOOtra2ouYVDABARqFQuqMYnZ2dMWXKlFi2bFlR9+3YsSN27drVfYwcObKo+8tSJujq6oqurq4e5wqFQuRyaaVlAKhMpewZ6O07r6qqKqqqqg67dt68eTFv3ryi5xg5cmS8/e1vP9Yllj4z8Kc//Sk+9rGPHfGa5ubmGDZsWI+jkH+11EsBgGNSKORKdvT2ndfc3FzS9U6dOjVGjRoVF154YfzqV78q+v6SBwN/+9vf4jvf+c4Rr2lsbIyOjo4eR27AkFIvBQDKrrfvvMbGxpKMPWrUqFi+fHn88Ic/jB/+8Icxbty4mD17djz11FNFjVN0meDHP/7xET9/7rnnjjpGb+kRJQIAKkUpywR9lQRK4dRTT41TTz21++eZM2fGH/7wh7j11lvju9/9br/HKToYqKuri1wuF4UjdEX4YgfgjeyN/ADCs88+OzZu3FjUPUWXCUaNGhU/+tGPIp/P93oUm5oAAEpn27ZtMWrUqKLuKTozMG3atNiyZUvMnz+/18+PljUAgEpXricQ7t27N5599tnun59//vnYtm1bDB8+PE4++eRobGyMF198Me67776IiLjtttti/Pjx8d73vjf2798f3/72t+Pxxx+PX/ziF0XNW3Qw8NnPfjY6Ozv7/HzChAnxy1/+sthhAaBilOsJhJs3b44LLrig++clS5ZERMTChQujpaUldu3aFTt37uz+/MCBA/HpT386XnzxxTjppJNi8uTJ8dhjj/UYoz9yhQr5M/4tA8eUewlUkH1/+Z9yL4EK8rVp/1XuJVBh/vOF/3dcx/9VzYdKNta5bQ+VbKzjxbsJACAjX+4FvM4EAwCQUYi0dsV5NwEAJE5mAAAy8hXRTff6EQwAQEY+sTKBYAAAMvQMAABJkRkAgAxbCwEgccoEAEBSZAYAIEOZAAASl1owoEwAAImTGQCAjNQaCAUDAJCRTysWUCYAgNTJDABAhncTAEDiEntpoWAAALJsLQQAkiIzAAAZ+ZyeAQBIWmo9A8oEAJA4mQEAyEitgVAwAAAZnkAIACRFZgAAMjyBEAASZzcBAJAUmQEAyEitgVAwAAAZthYCQOL0DAAASZEZAIAMPQMAkLjUegaUCQAgcTIDAJCRWmZAMAAAGYXEegaUCQAgcTIDAJChTAAAiUstGFAmAIDEyQwAQEZqjyMWDABAhicQAkDi9AwAAEmRGQCAjNQyA4IBAMhIrYFQmQAAEiczAAAZqe0mkBkAgIx8CY9ibNiwIS655JIYPXp05HK5WLVq1VHvWbduXbzvfe+LqqqqmDBhQrS0tBQ5q2AAACpGZ2dnTJkyJZYtW9av659//vm4+OKL44ILLoht27bFddddF1dddVU8+uijRc2rTAAAGeVqIJw3b17Mmzev39cvX748xo8fH7fccktEREyaNCk2btwYt956a8ydO7ff4wgGACAjX8JwoKurK7q6unqcq6qqiqqqqn977NbW1pgzZ06Pc3Pnzo3rrruuqHEEA1SkwaPPK/cSqCB//cikci8Bjllzc3PccMMNPc41NTXF9ddf/2+P3dbWFtXV1T3OVVdXx549e2Lfvn0xePDgfo0jGACAjFI+dKixsTGWLFnS41wpsgKlJBgAgIxS9gyUqiTQm5qammhvb+9xrr29PYYOHdrvrECEYAAADvNGeRxxbW1tPPLIIz3OrVmzJmpra4sax9ZCAKgQe/fujW3btsW2bdsi4p9bB7dt2xY7d+6MiH+WHOrr67uv/8QnPhHPPfdcfO5zn4vf/va38c1vfjN+8IMfxKc+9ami5pUZAICMcj2BcPPmzXHBBRd0//y/vQYLFy6MlpaW2LVrV3dgEBExfvz4+OlPfxqf+tSn4vbbb4+xY8fGt7/97aK2FUYIBgDgMKXcWliM2bNnR6HQ99y9PV1w9uzZsXXr1n9rXmUCAEiczAAAZKT2CmPBAABkvFF2E5SKMgEAJE5mAAAyytVAWC6CAQDISCsUUCYAgOTJDABARmoNhIIBAMjQMwAAiUsrFNAzAADJkxkAgAw9AwCQuEJihQJlAgBInMwAAGQoEwBA4lLbWqhMAACJkxkAgIy08gKCAQA4jDIBAJAUmQEAyLCbAAASl9pDhwQDAJCRWmZAzwAAJE5mAAAylAkAIHHKBABAUmQGACAjX1AmAICkpRUKKBMAQPJkBgAgI7V3EwgGACAjta2FygQAkDiZAQDISO05A4IBAMjQMwAAidMzAAAkRWYAADL0DABA4gqJPY5YmQAAEiczAAAZdhMAQOJS6xlQJgCAxMkMAEBGas8ZEAwAQEZqPQPKBACQOJkBAMhI7TkDggEAyEhtN4FgAAAyUmsg1DMAAImTGQCADLsJACBxhUKhZEexli1bFqecckoMGjQozjnnnHjyySf7vLalpSVyuVyPY9CgQUXPKRgAgArxwAMPxJIlS6KpqSmeeuqpmDJlSsydOzdeeumlPu8ZOnRo7Nq1q/t44YUXip5XMAAAGfkolOwoxje+8Y24+uqr48orr4zTTjstli9fHieddFLcc889fd6Ty+Wipqam+6iuri7631t0MLBv377YuHFj/OY3vznss/3798d999131DG6urpiz549PY7U9nQCULkKJfyvt++8rq6uw+Y8cOBAbNmyJebMmdN9bsCAATFnzpxobW3tc6179+6Nd73rXTFu3LiYP39+PPPMM0X/e4sKBn73u9/FpEmT4vzzz48zzjgjZs2aFbt27er+vKOjI6688sqjjtPc3BzDhg3rcRTyrxa9eACodL195zU3Nx923e7du+PQoUOH/WVfXV0dbW1tvY596qmnxj333BMPP/xwfO9734t8Ph8zZ86MP//5z0Wtsahg4POf/3ycfvrp8dJLL8WOHTtiyJAhce6558bOnTuLmrSxsTE6Ojp6HLkBQ4oaAwCOl3yhULKjt++8xsbGkqyztrY26uvrY+rUqTFr1qz40Y9+FO985zvjrrvuKmqcorYWbtq0KR577LEYMWJEjBgxIn7yk5/EJz/5yTjvvPPil7/8Zbz1rW/t1zhVVVVRVVXV41wulytmKQBw3JSycN3bd15vRowYESeccEK0t7f3ON/e3h41NTX9muvEE0+MM888M5599tmi1lhUZmDfvn3xlrf8K37I5XJx5513xiWXXBKzZs2K3/3ud0VNDgD808CBA2PatGmxdu3a7nP5fD7Wrl0btbW1/Rrj0KFD8fTTT8eoUaOKmruozMDEiRNj8+bNMWnSpB7nly5dGhERl156aVGTA0AlKtdDh5YsWRILFy6Ms846K84+++y47bbborOzs7sfr76+PsaMGdPdc3DjjTfGjBkzYsKECfHKK6/EzTffHC+88EJcddVVRc1bVDDwwQ9+ML7//e/HRz/60cM+W7p0aeTz+Vi+fHlRCwCASlOuYGDBggXx8ssvx5e//OVoa2uLqVOnxs9//vPupsKdO3fGgAH/Sur//e9/j6uvvjra2triHe94R0ybNi02bdoUp512WlHz5goVsqfvLQPHlHsJQIX660cmHf0ikjLs3seO6/gzRs8u2VhP/GVdycY6Xjx0CAAS50VFAJCR2ouKBAMAkFFILBhQJgCAxMkMAEBGhfTWv24EAwCQkVrPgDIBACROZgAAMpQJACBxygQAQFJkBgAgI7XnDAgGACAjr2cAANKWWmZAzwAAJE5mAAAylAkAIHHKBABAUmQGACBDmQAAEqdMAAAkRWYAADKUCQAgccoEAEBSZAYAIKNQyJd7Ca8rwQAAZOQTKxMIBgAgo5BYA6GeAQBInMwAAGQoEwBA4pQJAICkyAwAQIYnEAJA4jyBEABIiswAAGSk1kAoGACAjNS2FioTAEDiZAYAIEOZAAASZ2shACQutcyAngEASJzMAABkpLabQDAAABnKBABAUmQGACDDbgIASJwXFQEASZEZAIAMZQIASJzdBABAUmQGACAjtQZCwQAAZCgTAEDiCoVCyY5iLVu2LE455ZQYNGhQnHPOOfHkk08e8foHH3wwJk6cGIMGDYozzjgjHnnkkaLnFAwAQIV44IEHYsmSJdHU1BRPPfVUTJkyJebOnRsvvfRSr9dv2rQpLr/88li0aFFs3bo16urqoq6uLrZv317UvLlCheRC3jJwTLmXAFSov35kUrmXQIUZdu9jx3X8Un4ndb76XHR1dfU4V1VVFVVVVYdde84558T06dNj6dKlERGRz+dj3Lhxcc0118QXvvCFw65fsGBBdHZ2xurVq7vPzZgxI6ZOnRrLly/v/yILVIz9+/cXmpqaCvv37y/3UqgAfh94Lb8Pb1xNTU2FiOhxNDU1HXZdV1dX4YQTTiisXLmyx/n6+vrCpZde2uvY48aNK9x66609zn35y18uTJ48uag1KhNUkK6urrjhhhsOiyBJk98HXsvvwxtXY2NjdHR09DgaGxsPu2737t1x6NChqK6u7nG+uro62traeh27ra2tqOv7YjcBABxHfZUEKonMAABUgBEjRsQJJ5wQ7e3tPc63t7dHTU1Nr/fU1NQUdX1fBAMAUAEGDhwY06ZNi7Vr13afy+fzsXbt2qitre31ntra2h7XR0SsWbOmz+v7okxQQaqqqqKpqani00m8Pvw+8Fp+H9KwZMmSWLhwYZx11llx9tlnx2233RadnZ1x5ZVXRkREfX19jBkzJpqbmyMi4tprr41Zs2bFLbfcEhdffHHcf//9sXnz5rj77ruLmrdithYCABFLly6Nm2++Odra2mLq1Klxxx13xDnnnBMREbNnz45TTjklWlpauq9/8MEH40tf+lL88Y9/jHe/+93xta99LT7wgQ8UNadgAAASp2cAABInGACAxAkGACBxggEASJxgoEIU+8pK3rw2bNgQl1xySYwePTpyuVysWrWq3EuijJqbm2P69OkxZMiQGDlyZNTV1cWOHTvKvSzeZAQDFaDYV1by5tbZ2RlTpkyJZcuWlXspVID169dHQ0NDPPHEE7FmzZo4ePBgXHTRRdHZ2VnupfEmYmthBSj2lZWkI5fLxcqVK6Ourq7cS6FCvPzyyzFy5MhYv359nH/++eVeDm8SMgNlduDAgdiyZUvMmTOn+9yAAQNizpw50draWsaVAZWoo6MjIiKGDx9e5pXwZiIYKLNjeWUlkKZ8Ph/XXXddnHvuuXH66aeXezm8iXg3AcAbRENDQ2zfvj02btxY7qXwJiMYKLNjeWUlkJ7FixfH6tWrY8OGDTF27NhyL4c3GWWCMjuWV1YC6SgUCrF48eJYuXJlPP744zF+/PhyL4k3IZmBCnC0V1aSlr1798azzz7b/fPzzz8f27Zti+HDh8fJJ59cxpVRDg0NDbFixYp4+OGHY8iQId29RMOGDYvBgweXeXW8WdhaWCGO9MpK0rJu3bq44IILDju/cOHCHq8tJQ25XK7X8/fee29cccUVr+9ieNMSDABA4vQMAEDiBAMAkDjBAAAkTjAAAIkTDABA4gQDAJA4wQAAJE4wAACJEwwAQOIEAwCQOMEAACTu/wO7g4juNjpOcAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}